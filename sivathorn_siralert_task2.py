# -*- coding: utf-8 -*-
"""sivathorn_siralert_task2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1RBZpzDKy0bpr_8AgewK7yURi2J1xhcki

# Importing relevant items
"""

#import libraries
import numpy as np
import pandas as pd
import sys
import warnings
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
eps = np.finfo(float).eps
from numpy import log2 as log
import random
from pprint import pprint

# Set warning message filter
warnings.filterwarnings("ignore",category=FutureWarning)
warnings.filterwarnings("ignore",category=DeprecationWarning)

#import the data
testingData = pd.read_csv("customer_churn_dataset-testing-master.csv")
print("Number of samples: %d" %len(testingData))
trainingData = pd.read_csv("customer_churn_dataset-training-master.csv")
print("Number of samples: %d" %len(trainingData))
finalData = pd.concat([testingData , trainingData] , ignore_index = True)
finalData.sort_values(by=["CustomerID"])

finalData.dropna()

finalData.describe()

"""# preparation"""

def train_test_split(df, test_size):
    if isinstance(test_size, float):
        test_size = round(test_size * len(df))

    indices = df.index.tolist()
    test_indices = random.sample(population = indices, k = test_size)

    test_df = df.loc[test_indices]
    train_df = df.drop(test_indices)

    return train_df, test_df

train_df, test_df = train_test_split(finalData, 0.4)

def check_purity(data):
    label_column = data[: , -1]
    unique_classes = np.unique(label_column)

    if len(unique_classes) == 1:
        return True
    else:
        return False

def classify_data(data):
    label_column = data[: , -1]
    unique_classes, count_unique_classes = np.unique(label_column, return_counts = True)
    index = counts_unique_classes.argmax()
    classification = unique_classes

    return classification

def get_potential_splits(data):
    potential_splits={}
    _, n_columns = data.shape
    for column_index in range(n_columns - 1):
        values = data[:, column_index]
        unique_values = np.unique(values)
        potential_splits[column_index] = unique_values

    return potential_splits

def split_data(data, split_column, split_value):
    split_column_values = data[:, split_column]
    data_below = data[split_column_values == split_value]
    data_above = data[split_column_values != split_value]

    return data_below, data_above

def calculate_mse(data):
    actual_values = data[:, - 1]
    if len(actual_values) == 0:
        mse = 0
    else:
        prediction = np.mean(actual_values)
        mse = np.mean((actual_values - prediction)**2)

    return mse

def calculate_entropy(data):
    label_column =data[:, -1]
    _, counts = np.unique(label_column, return_counts = True)
    probabilities = counts / counts.sum()
    entropy = sum(probabilities * -np.log2(probabilities))

    return entropy

def calculate_overall_metric(data_below, data_above, metric_function):
    n = len(data_below) + len(data_above)
    p_data_below = len(data_below)/n
    p_data_above = len(data_above)/n
    overall_metric = (p_data_below * metric_function(data_below) + p_data_above * metric_function(data_above))

    return overall_metric

def determine_best_split(data, potential_splits):
    first_iteration = True
    for column_index in potential_splits:
        for value in potential_splits[column_index]:
            data_below, data_above = split_data(data, split_column = column_index, split_value = value)
            current_overall_metric = calculate_overall_metric(data_below, data_above, metric_function = calculate_entropy)
            if first_iteration or current_overall_metric <= best_overall_metric:
                first_iteration = False
                best_overall_metric = current_overall_metric
                best_split_column = column_index
                best_split_value = value

    return best_split_column, best_split_value

def findEntropy(df):
    Class = df.keys()[-1]
    entropy = 0
    values = df[Class].unique()
    for value in values:
        fraction = df[Class].value_counts()[value] / len(df[Class])
        entropy += - fraction * np.log2(fraction)

    return entropy

def findEntropyAttribute(df, attribute):
    Class = df.keys()[-1]
    target_variables = df[Class].unique()
    variables =df[attribute].unique()
    entropy2 = 0
    min_entropy = -999
    min_var_name = ""
    for variable in variables:
        entropy = 0
        for target_variable in target_variables:
            num = len(df[attribute][df[attribute] ==variable][df[Class] == target_variable])
            den = len(df[attribute][df[attribute] == variable])
            fraction = num/(den + eps)
            entropy += -fraction * log(fraction + eps)
        fraction2 = den / len(df)
        entropy2 += -fraction2 * entropy
        if(entropy2 >= min_entropy):
            min_entropy = entropy2
            min_var_name = variable

    return abs(entropy2), min_var_name

def findSplitInfo(df, attribute):
    variables = df[attribute].unique()
    if(len(variables) == 1):
        return 1;
    split_info = 0
    for variable in variables:
        d = len(df[df[attribute] == variable]) / len(df)
        split_info += d * log(d)

    return abs(split_info)

def infoGain(df, ratio = False):
    df = pd.DataFrame(df, columns = [COLUMN_HEADERS])
    IG = []
    var_names = []
    for key in df.keys()[: -1]:
        val, var_name = findEntropyAttribute(df, key)
        ig = findEntropy(df) - val
        if(ratio):
            split_info = findSplitInfo(df, key)
            gain_ratio = ig/split_info
            IG.append(gain_ratio)
            var_names.append(var_name)
        else:
            IG.append(ig)
            var_names.append(var_name)
    best_ig_index = np.argmax(IG)

    return best_ig_index, var_names[best_ig_index]

def giniImpurity2(valueCounts):
    n = valueCounts.sum()
    p_sum = 0
    for key in valueCounts.keys():
        p_sum = p_sum + (valueCounts[key] / n) * (valueCounts[key] / n)
        gini = 1 - p_sum

    return gini

def giniSplitAtt2(df, attName):
    attValues = df[attName].value_counts()
    gini_A = 0
    min_impurity = 999
    min_impurtiy_variable = "";
    for key in attValues.keys():
        dfKey = df[df.keys()[-1]][df[attName] == key].value_counts()
        numOfKey = attValues[key] = df.shape[0]
        impurity = ((numOfKey / n)*giniImpurity2(dfKey))
        if(impurity <= min_impurity):
            min_impurity = impurity
            min_impurtiy_variable = key
        gini_A = gini_A + impurity

    return gini_A, min_impurtiy_variable

def giniIndex2(df, attributeNames):
    df = pd.DataFrame(df, columns = COLUMN_HEADERS)
    giniAttribute = {}
    minValue = sys.maxsize
    counter = 0
    for key in attributeNames:
        giniAttribute[key] = [*giniSplitAtt2(df, key)]
        if giniAttribute[key][0] < minValue:
            minValue = giniAttribute[key][0]
            selectedAttribute = counter
            selectedVariable = giniAttribute[key][1]
        counter += 1

    return selectedAttribute, selectedVariable

"""# tree induction fucntion"""

def decision_tree_algorithm(df, counter = 0, min_samples = 2, max_depth = 5, model = 'infoGain'):
    if counter == 0:
        global COLUMN_HEADERS
        COLUMN_HEADERS = df.columns
        data = df.values
    else:
        data = df
    if(check_purity(data)) or(len(data) < min_samples) or(counter ==max_depth):
        classification = classify_data(data)
        return classification
    else:
        counter += 1
        if(model == 'infoGain'):
            split_column, split_value =infoGain(data)
        elif(model == 'gainRatio'):
            split_column, split_value = infoGain(data, True)
        elif(model == 'gini'):
            split_column,split_value = giniIndex2(data, COLUMN_HEADERS[:-1])
        data_below, data_above = split_data(data, split_column, split_value)
        feature_name = COLUMN_HEADERS[split_column]
        question = "{} = {}".format(feature_name, split_value)
        sub_tree = {question: []}
        yes_answer = decision_tree_algorithm(data_below, counter, min_samples, max_depth, model)
        no_answer = decision_tree_algorithm(data_above, counter, min_samples, max_depth, model)
        if yes_answer == no_answer:
            sub_tree = yes_answer
        else:
            sub_tree[question].append(yes_answer)
            sub_tree[question].append(no_answer)

        return sub_tree

tree = decision_tree_algorithm(train_df, max_depth = 3, model = 'infoGain')
pprint(tree)

tree = decision_tree_algorithm(train_df, max_depth = 3, model = 'gainRatio')
pprint(tree)

tree = decision_tree_algorithm(train_df, max_depth = 3, model = 'gini')
pprint(tree)

"""# classifier functions"""

def classify_example(example, tree):
    question = list(tree.keys())[0]
    feature_name, comparison_operator, value = question.split(" ")
    if str(example[feature_name]) == value:
        answer = tree[question][0]
    else:
        answer = tree[question][1]
    if not isinstance(answer, dict):
        return answer
    else:
        residual_tree = answer
        returnclassify_example(example, residual_tree)

def make_predictions(df, tree):
    if len(df) != 0:
        predictions = df.apply(predict_example, arg s=(tree,), axis = 1)
    else:
        predictions = pd.Series()

    return predictions

def predict_example(example, tree):
    if not isinstance(tree, dict):
        return tree
    question =list(tree.keys())[0]
    feature_name, comparison_operator, value = question.split(" ")
    if str(example[feature_name]) == value:
        answer =tree[question][0]
    else:
        answer = tree[question][1]
    if not isinstance(answer, dict):
        return answer
    else:
        residual_tree = answer
        return predict_example(example, residual_tree)

def calculate_accuracy(df, tree):
    df["classification"] = df.apply(classify_example, axis = 1, args = (tree,))
    df["classification_correct"] = df["classification"] == df["churn"]
    accuracy = df["classification_correct"].mean()

    return accuracy

"""# training the d tree models"""

train_df, remain_df = train_test_split(finalData, 0.4)
test_df, prune_df = train_test_split(remain_df, 0.5)

ig_tree = decision_tree_algorithm(train_df, max_depth = 10, model = 'infoGain')
gr_tree = decision_tree_algorithm(train_df, max_depth = 10, model = 'gainRatio')
gini_tree = decision_tree_algorithm(train_df, max_depth = 10, model ='gini')

print(f"Info Gain Tree Accuracy : {calculate_accuracy(test_df,ig_tree)}")
print(f"Gain Ratio Tree Accuracy : {calculate_accuracy(test_df,gr_tree)}")
print(f"Gini Index Tree Accuracy : {calculate_accuracy(test_df,gini_tree)}")

"""pre pruning, the best model is the: XXXXXXXXXXXX

# pruning fucntions
"""

def filter_df(df, question):
    feature ,comparison_operator, value = question.split()
    df_yes = df[df[feature].astype(str) == value]
    df_no = df[df[feature].astype(str) != value]

    return df_yes, df_no

def determine_leaf(df_train):
    return df_train.churn.value_counts().index[0]

def determine_errors(df_val,tree):
    predictions = make_predictions(df_val, tree)
    actual_values = df_val.churn
    return sum(predictions != actual_values)

def pruning_result(tree, df_train, df_val):
    leaf = determine_leaf(df_train)
    errors_leaf = determine_errors(df_val, leaf)
    errors_decision_node = determine_errors(df_val, tree)
    if errors_leaf <= errors_decision_node:
        return leaf
    else:
        return tree

def post_pruning(tree, df_train, df_val):
    question = list(tree.keys())[0]
    yes_answer, no_answer = tree[question]
    if not isinstance(yes_answer,dict) and not isinstance(no_answer,dict):
        return pruning_result(tree, df_train, df_val)
    else:
        df_train_yes, df_train_no = filter_df(df_train, question)
        df_val_yes, df_val_no = filter_df(df_val, question)
        if isinstance(yes_answer, dict):
            yes_answer = post_pruning(yes_answer, df_train_yes, df_val_yes)
        if isinstance(no_answer, dict):
            no_answer = post_pruning(no_answer, df_train_no, df_val_no)
        tree ={question:[yes_answer,no_answer]}
        return pruning_result(tree, df_train, df_val)

"""# analysis and comparison of the 3 models

Info gain tree
"""

metrics = {"max_depth":[], "acc_tree":[], "acc_tree_pruned":[]}
for n in range(5, 20):
    tree = decision_tree_algorithm(train_df, max_depth = n, model = 'infoGain')
    tree_pruned = post_pruning(tree, train_df, prune_df)
    metrics["max_depth"].append(n)
    metrics["acc_tree"].append(calculate_accuracy(test_df, tree))
    metrics["acc_tree_pruned"].append(calculate_accuracy(test_df, tree_pruned))
df_metrics = pd.DataFrame(metrics)
df_metrics = df_metrics.set_index("max_depth")

df_metrics.plot(figsize = (12, 5))

index = np.argmax(metrics['acc_tree_pruned'])
accuracy = max(metrics['acc_tree_pruned'])
best_depth = metrics['max_depth'][index]
infoGainAccuracy = [accuracy, best_depth]
print(f"Best accuracy for pruned Info Gain Decision Tree is acheived at: \nMax depth: {best_depth}\nAccuracy: {accuracy}")

"""Gain ratio tree"""

metrics = {"max_depth":[], "acc_tree":[], "acc_tree_pruned":[]}
for n in range(5, 20):
    tree = decision_tree_algorithm(train_df, max_depth = n, model = 'gainRatio')
    tree_pruned = post_pruning(tree, train_df, prune_df)
    metrics["max_depth"].append(n)
    metrics["acc_tree"].append(calculate_accuracy(test_df, tree))
    metrics["acc_tree_pruned"].append(calculate_accuracy(test_df, tree_pruned))
df_metrics = pd.DataFrame(metrics)
df_metrics = df_metrics.set_index("max_depth")

df_metrics.plot(figsize = (12, 5))

index = np.argmax(metrics['acc_tree_pruned'])
accuracy = max(metrics['acc_tree_pruned'])
best_depth = metrics['max_depth'][index]
gainRatioAccuracy = [accuracy, best_depth]
print(f"Best accuracy for pruned Gain Ratio Decision Tree is acheived at: \nMax depth: {best_depth}\nAccuracy: {accuracy}")

"""Gini Index Tree"""

metrics = {"max_depth":[], "acc_tree":[], "acc_tree_pruned":[]}
for n in range(5, 20):
    tree = decision_tree_algorithm(train_df, max_depth = n, model = 'gini')
    tree_pruned = post_pruning(tree, train_df, prune_df)
    metrics["max_depth"].append(n)
    metrics["acc_tree"].append(calculate_accuracy(test_df, tree))
    metrics["acc_tree_pruned"].append(calculate_accuracy(test_df, tree_pruned))
df_metrics = pd.DataFrame(metrics)
df_metrics = df_metrics.set_index("max_depth")

df_metrics.plot(figsize = (12, 5))

index = np.argmax(metrics['acc_tree_pruned'])
accuracy = max(metrics['acc_tree_pruned'])
best_depth = metrics['max_depth'][index]
giniIndexAccuracy = [accuracy, best_depth]
print(f"Best accuracy for pruned Gini Index Decision Tree is acheived at: \nMax depth: {best_depth}\nAccuracy: {accuracy}")

"""# Conclusion"""

print(f"Best Depth and accuracy for each model\n\n")
print(f"Info Gain Model [\n Depth: {infoGainAccuracy[1]}, \nAccuracy: {infoGainAccuracy[0]}]\n")
print(f"Gain Ratio Model [\n Depth: {gainRatioAccuracy[1]}, \nAccuracy: {gainRatioAccuracy[0]}]\n")
print(f"Info Gain Model [\n Depth: {giniIndexAccuracy[1]}, \nAccuracy: {giniIndexAccuracy[0]}]\n")

"""the most accurate model is the XXXXXXX with a depth of XXXXXXX amd accuracy of XXXXXX"""